%% Marmalade — Implementation-accurate AI Flows & ElevenLabs Hooks
%% File: docs/architecture-ai-flows.mmd

# Hooks Integration (ElevenLabs agents & external callers)

```mermaid
flowchart LR
  external[ElevenLabs Agent / Custom LLM]
  hooks[Hooks: POST /v1/chat/completions<br/>(server/src/routes/hooks.route.ts)]
  prepare[prepareTurn: ensureSession + persist user message (message.repo)]
  conv[ConversationService]
  stream[Server SSE / event stream]

  external -->|POST chat completion request| hooks
  hooks --> prepare
  prepare --> conv
  conv -->|handleUserTurnModelStream| conv
  conv -->|streaming chunks| hooks
  hooks -->|SSE chunks| external

  classDef info fill:#e8f7ff,stroke:#7cc0ff
  note[(Note: Supports streaming and non-streaming; headers x-user-id / x-session-id supported.)]:::info
  ElevenNote[(ElevenLabs media flow: The client uses `useConversation` (`client/src/features/session/hooks/use-elevenlabs.session.ts`). Live audio/text streams go directly between the browser and ElevenLabs (WebRTC/streaming) — Marmalade does NOT proxy media. The backend handles session lifecycle (create/end internal session ids), receives turns from the ElevenLabs agent via `/v1/chat/completions`, persists messages, and emits SSE events for the UI.)]:::info
```

# Sequence: ElevenLabs incoming request (streaming)

```mermaid
sequenceDiagram
  participant Agent as ElevenLabs Agent
  participant Hooks as /v1/chat/completions
  participant Prepare as prepareTurn
  participant Conv as ConversationService
  participant Mini as MiniBrain
  participant Coord as TurnCoordinator
  participant Coun as Counselor
  participant DB as DB

  Agent->>Hooks: POST /v1/chat/completions {messages, stream=true}
  Hooks->>Prepare: ensure session + persist message
  Prepare->>DB: write user message
  Prepare-->>Hooks: session ready
  Hooks->>Conv: start handleUserTurnModelStream(session.id, transcript)
  Conv->>Mini: analyzeTurn (fast state read)
  Mini-->>Conv: stateRead, stateDelta
  Conv->>Coord: coordinateTurn(graph, stateRead, signals)
  Coord->>Coun: request reply (system instruction + preferences)
  Coun-->>Conv: streaming reply chunks
  Conv-->>Hooks: stream chunks back
  Hooks-->>Agent: SSE chunked responses
  Conv->>DB: persist updated graph & signals
```

# AI Flows: which model, when, and how

```mermaid
flowchart TB
  User[User message]
  Mini[MiniBrain: fast analysis]
  Coord[Turn Coordinator: deterministic rules]
  Counselor[Counselor LLM]
  First[FirstResponse]
  RAG[EmbeddingRepo / RAG]

  User --> Mini
  Mini --> Coord
  Coord --> RAG
  Coord --> Counselor
  Coord --> First
  Counselor -->|final reply| User
```

## How-to (operational notes)

- MiniBrain (fast analysis)
  - Use for: immediate safety checks, building `stateRead` and `stateDelta`, suggesting probes.
  - Response must be validated with zod (already done in mini-brain client) and keep outputs small.
  - **New (Orchestration)**: MiniBrain returns an authoritative boolean `requiresCounselor` which **gates** whether the Counselor model should run for this turn; this prevents double-responses and reduces cost/latency when not needed.

- Turn Coordinator
  - Deterministic rules (server): `willStatus` reduces threshold for grounding; `interactionPreference` adjusts language plan; `lifeAnchors` bias anchoring.
  - Ensures the system has a consistent plan before calling larger LLMs.

- Counselor (main LLM)
  - Use for: generating the substantive reply with access to RAG, system instructions tuned for `languagePlan` and `decision`.
  - Inputs: systemInstruction (pinned), conversation window, compressed RAG results, `preferences` (graph + signals + language plan + stateRead + stateDelta).

- Hooks route specifics
  - Headers: `x-user-id` (preferred) or `user_id` in body; `session_id` accepted for client session mapping.
  - Streaming: SSE with `chat.completion.chunk` events (compatible with ElevenLabs agents). Non-streaming returns JSON chat.completion.

- Safety
  - Always enforce coarse gating server-side (`intervention-arbiter`) — do not delegate grounding eligibility to LLM prompt only.
  - Use `systemInstruction` templates that encode `interactionPreference`/`willStatus` so model output is aligned with deterministic plan.

---

## Implementation-accurate details (current code)

### Streaming orchestration (key points)
- `handleUserTurnModelStream` orchestrates two concurrent streams:
  - `FirstResponse` stream (fast, short, friendly) which starts immediately and yields small chunks.
  - `Counselor` stream (pro reply) starts when either the FirstResponse finishes or emittedWordCount >= `thresholdWords` (8), to balance responsiveness and quality.
  - **Speculative execution note**: MiniBrain and RAG are started in parallel with FirstResponse; the Counselor can be warmed-up speculatively (small warm-up + bounded buffer) and **only** flushed to the client if MiniBrain's `requiresCounselor` is true (this reduces perceived latency while preventing unnecessary counselor runs).
- Chunks from both generators are raced and emitted to the caller (SSE or OpenAI-style `chat.completion.chunk`).
- The function uses `finalizeAndSaveAsync` to persist the final assistant reply asynchronously so streaming doesn't block DB writes.
- **Logging**: key decision points (Mini result, coordinator decision, RAG, counselor warm-up and first-chunk latency, final save) are emitted via the app's structured pino logger for demo and observability.

### MiniBrain & safety
- `MiniBrain.analyzeTurn` (in `server/src/libs/ai/mini-brain.client.ts`) is the canonical per-turn classifier. Outputs are validated with Zod (`miniResponseSchema`).
- Risk range is 0–4. The code treats `mini.riskLevel > 3` as a high-risk trigger and uses a crisis-safe canned reply instead of calling the counselor model.
- `buildMiniFallbackResult` detects crisis language via a regex (e.g., `suicide|suicidal|kill|end it`) and returns `riskLevel: 4` immediately.
- The `getSafetyMode` helper maps risk → `normal|caution|high_caution|crisis` (>=4→crisis, 3→high_caution, 2→caution, else normal).

### Deterministic coordination
- `coordinateTurn` (turn-coordinator) uses user graph & signals (e.g., `willStatus`, `interactionPreference`) to decide:
  - `responseClass` (understanding/reflection/anchoring/grounding)
  - `groundingEligible` and `groundingReason`
  - `languagePlan` (sentence length, rawness, metaphor density)
- These deterministic outputs are sent as part of `preferences` to the counselor LLM in `systemInstruction` to keep model output aligned.

### Persistence & post-turn updates
- Each resolved turn updates:
  - `conversation_state` (summary, mood, riskLevel, lastThemes)
  - `risk_logs` (per-turn risk and themes)
  - `voice_sessions` (max risk via `sessions.updateMaxRisk`, message count)
  - `messages` table (assistant message with `riskAtTurn`, `themes`, `metadata`)
- Session end flow (`SessionService.endSession`) calls `summarizeSession`, which:
  - Aggregates messages + risks, computes `riskSummary` (min/max/avg), extracts `topThemes`.
  - Builds `summaryContent` JSON, generates an embedding via `EmbeddingClient`, and creates a `memory_docs` record (`type: session_summary`) via `MemoryDocRepository`.

### Telemetry & fallback behavior
- AI clients log trimmed candidate metadata and token usage when available to help with cost/monitoring.
- `withTimeout` wraps long model calls so the system can fall back to `EMERGENCY_PACKET` (a conservative assistant message) if a call stalls.
- Zod validation errors are surfaced as `AppError` with codes such as `INVALID_MINI_RESPONSE_SCHEMA` and handled safely by the pipeline.

### Hooks route precise behavior
- `server/src/routes/hooks.route.ts` implements `/v1/chat/completions` used by ElevenLabs or other agents.
  - Accepts `messages` array (OpenAI-like), extracts the last user `transcript` (string). Supports `stream: true` (SSE) or `false` (single response JSON).
  - Uses headers `x-user-id` / `x-session-id` or body `user_id` / `session_id`. Non-UUID `session_id` values are mapped to internal session ids via `externalSessionToInternal`.
  - Persists the incoming user message before invoking `ConversationService` to handle the turn.
  - For streaming calls it sends incremental `chat.completion.chunk` events as the service yields chunks.
- Default fallback user: `ELEVENLABS_DEFAULT_USER_ID` env var.
- Security note: the hook is convenient for integration but is unauthenticated by default — add an API key / signed secret header for production use (e.g., validate `ELEVENLABS_WEBHOOK_SECRET`).

---

## Quick code pointers
- Streaming orchestration: `server/src/services/conversation.service.ts` → `handleUserTurnModelStream`
- Mini classifier: `server/src/libs/ai/mini-brain.client.ts`
- Counselor: `server/src/libs/ai/counselor-brain.client.ts` (streaming + JSON output)
- Hooks: `server/src/routes/hooks.route.ts` (OpenAI-compatible endpoint)
- RAG: `server/src/repositories/embedding.repository.ts` and `server/src/libs/ai/embedding.client.ts`
- Persistence: `server/src/repositories/*` for messages, sessions, risk logs, memory docs

---

## Report (SOAP note) — where it lives & how it works

```mermaid
flowchart LR
  client[Client or Caller]
  reportsApi[POST /api/reports/session<br/>(server/src/routes/reports.route.ts)]
  reportService[ReportService.generateSessionReport<br/>(server/src/services/report.service.ts)]
  sessionReportClient[SessionReportClient<br/>(server/src/libs/ai/session-report.client.ts)]
  vertex[Vertex: VERTEX_COUNSELOR_MODEL]
  types[Types: shared/src/types/report.type.ts]

  client -->|POST {sessionId}| reportsApi
  reportsApi --> reportService
  reportService --> sessionReportClient
  sessionReportClient --> vertex
  vertex --> sessionReportClient
  sessionReportClient --> reportService
  reportService -->|returns| client

  classDef info fill:#fff4e6,stroke:#ffbe7a
  note[(Note: The report generator enforces strict JSON (Zod) and returns a SOAP-style `report.soapNote`. By default the report is returned to the caller and is not persisted.)]:::info
```

- Endpoint: `POST /api/reports/session` (auth required). Implemented in `server/src/routes/reports.route.ts` and validated by `server/src/routes/validators/reports.validator.ts`.
- Service: `ReportService.generateSessionReport(...)` collects recent messages and conversation state, constructs `systemInstruction`, and calls the `SessionReportClient`.
- AI client: `SessionReportClient.generateReport(...)` uses `VERTEX_COUNSELOR_MODEL`, instructs it to return the `REPORT_OUTPUT_SCHEMA` (strict JSON), and validates the response with Zod (`reportSchema`). See `server/src/libs/ai/session-report.client.ts`.
- Types: `shared/src/types/report.type.ts` defines `ConversationReport` and the `soapNote` S/O/A/P structure.
- Persistence: Currently the service returns the report to the client (no DB storage). You can easily persist it by:
  - Saving the JSON in a new `reports` table (recommended for history/audit), or
  - Attaching the report JSON or `reportDocId` to the `voice_sessions` row at session end.
- Optional workflow improvements:
  - Auto-generate at session end inside `SessionService.endSession` (sync or background job), and emit a `report-ready` event on the bus for the client to fetch.
  - Add `GET /api/reports/:id` and a small repository (`ReportRepository`) to list persisted reports.

---